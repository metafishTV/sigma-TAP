# TAPS Stage 2 Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build parameter sensitivity analysis, mode transition maps, gated vs ungated divergence finder, texture validation, and correlation stability analysis for the TAPS diagnostic framework.

**Architecture:** New module `simulator/taps_sensitivity.py` (pure functions, testable) + CLI driver `scripts/taps_stage2.py`. Follows Stage 1 pattern (taps.py -> taps_diagnostics.py). All computation is post-hoc from trajectory data. Does not modify existing scripts.

**Tech Stack:** Python 3.12, numpy, matplotlib, existing simulator infrastructure. No new external dependencies.

**Claim policy:** All outputs labeled `exploratory`.

---

### Task 1: Mode Transition Map — Core Classification & Counting

The transition map is the novel contribution: categorical state classification at each step, tracking structural movement stripped of quantitative values.

**Files:**
- Create: `simulator/taps_sensitivity.py`
- Create: `tests/test_taps_sensitivity.py`

**Step 1: Write failing tests for classify_step and build_transition_map**

```python
# tests/test_taps_sensitivity.py
"""Tests for TAPS Stage 2 sensitivity and transition analysis."""
import os
import sys
import unittest

import numpy as np

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))


def _make_trajectory(steps=10, with_events=True):
    """Build a synthetic trajectory for testing (same helper as test_taps.py)."""
    traj = []
    cumul = {"self": 0, "absorptive": 0, "novel": 0, "env": 0, "disint": 0,
             "types_lost": 0, "k_lost": 0.0, "deep_stasis": 0}
    for t in range(steps):
        if with_events and t == 3:
            cumul["self"] += 1
            cumul["absorptive"] += 1
        if with_events and t == 6:
            cumul["novel"] += 1
            cumul["disint"] += 1
        traj.append({
            "step": t,
            "D_total": 10 + t,
            "k_total": 50.0 + t * 5,
            "total_M": 100.0 + t * 10,
            "n_active": 8,
            "n_dormant": 2,
            "agent_k_list": [10.0] * 8,
            "convergence": 0.5,
            "texture_type": 1,
            "a_env": 3.0,
            "K_env": 2e5,
            "innovation_potential": 0.8 - t * 0.01,
            "n_self_metatheses": cumul["self"],
            "n_absorptive_cross": cumul["absorptive"],
            "n_novel_cross": cumul["novel"],
            "n_env_transitions": cumul["env"],
            "n_disintegration_redistributions": cumul["disint"],
            "n_types_lost": cumul["types_lost"],
            "k_lost": cumul["k_lost"],
            "n_deep_stasis": cumul["deep_stasis"],
            "affordance_mean": 0.6,
            "temporal_state_counts": {0: 0, 1: 2, 2: 3, 3: 3, 4: 2},
        })
    return traj


class TestClassifyStep(unittest.TestCase):
    """Tests for per-step categorical classification."""

    def test_classify_returns_all_axes(self):
        """classify_step should return labels for all 6 classification axes."""
        from simulator.taps_sensitivity import classify_step
        from simulator.taps import compute_all_scores, compute_anopression, compute_rip, pressure_ratio

        traj = _make_trajectory(steps=10)
        all_scores = compute_all_scores(traj, mu=0.005)
        ano_scores = compute_anopression(traj, mu=0.005)
        rip_result = compute_rip(traj)
        ratios = pressure_ratio(ano_scores)

        result = classify_step(all_scores, ano_scores, rip_result, ratios, step=5)
        expected_axes = {"rip_dominance", "pressure_regime", "texture_type",
                         "ano_dominant", "syntegration_phase", "transvolution_dir"}
        self.assertEqual(set(result.keys()), expected_axes)

    def test_classify_returns_strings(self):
        """All classification values should be strings."""
        from simulator.taps_sensitivity import classify_step
        from simulator.taps import compute_all_scores, compute_anopression, compute_rip, pressure_ratio

        traj = _make_trajectory(steps=10)
        all_scores = compute_all_scores(traj, mu=0.005)
        ano_scores = compute_anopression(traj, mu=0.005)
        rip_result = compute_rip(traj)
        ratios = pressure_ratio(ano_scores)

        result = classify_step(all_scores, ano_scores, rip_result, ratios, step=5)
        for axis, label in result.items():
            self.assertIsInstance(label, str, f"Axis {axis} should return str, got {type(label)}")


class TestTransitionMap(unittest.TestCase):
    """Tests for transition counting."""

    def test_constant_state_only_self_transitions(self):
        """If classification never changes, all transitions are self-loops."""
        from simulator.taps_sensitivity import build_transition_map
        from simulator.taps import compute_all_scores, compute_anopression, compute_rip, pressure_ratio

        # No events -> state stays constant (mostly)
        traj = _make_trajectory(steps=20, with_events=False)
        all_scores = compute_all_scores(traj, mu=0.005)
        ano_scores = compute_anopression(traj, mu=0.005)
        rip_result = compute_rip(traj)
        ratios = pressure_ratio(ano_scores)

        maps = build_transition_map(all_scores, ano_scores, rip_result, ratios, traj)

        # For RIP dominance with no events: always recursion -> only self-transitions
        rip_map = maps["rip_dominance"]
        # Sum off-diagonal should be 0 (or very small if step 0 is different)
        total_transitions = rip_map["counts"].sum()
        self.assertGreater(total_transitions, 0, "Should have some transitions")

    def test_transition_counts_sum_to_steps_minus_one(self):
        """Total transitions for each axis should equal steps - 1."""
        from simulator.taps_sensitivity import build_transition_map
        from simulator.taps import compute_all_scores, compute_anopression, compute_rip, pressure_ratio

        traj = _make_trajectory(steps=15, with_events=True)
        all_scores = compute_all_scores(traj, mu=0.005)
        ano_scores = compute_anopression(traj, mu=0.005)
        rip_result = compute_rip(traj)
        ratios = pressure_ratio(ano_scores)

        maps = build_transition_map(all_scores, ano_scores, rip_result, ratios, traj)

        for axis_name, axis_data in maps.items():
            total = axis_data["counts"].sum()
            self.assertEqual(total, 14,
                             f"Axis {axis_name}: expected 14 transitions (15-1), got {total}")


if __name__ == "__main__":
    unittest.main()
```

**Step 2: Run tests to verify they fail**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py -v`

Expected: FAIL with `ModuleNotFoundError: No module named 'simulator.taps_sensitivity'`

**Step 3: Implement classify_step and build_transition_map**

```python
# simulator/taps_sensitivity.py
"""TAPS Stage 2: parameter sensitivity, mode transitions, divergence analysis.

CLAIM POLICY LABEL: exploratory
Extends the TAPS diagnostic overlay with comparative and parametric analysis.
Pure functions operating on trajectory and score data — no simulation coupling.

See docs/plans/2026-02-26-taps-stage2-design.md for full design rationale.
"""
from __future__ import annotations

from typing import Any

import numpy as np

from simulator.taps import (
    Trajectory,
    compute_all_scores,
    compute_anopression,
    compute_rip,
    pressure_ratio,
)


# ---------------------------------------------------------------------------
# WI-1b: Mode Transition Map
# ---------------------------------------------------------------------------

def classify_step(
    all_scores: dict[str, list[float]],
    ano_scores: dict[str, list[float]],
    rip_result: dict[str, list],
    ratios: list[float],
    step: int,
) -> dict[str, str]:
    """Classify system state at one step across all categorical axes.

    Returns dict with one string label per axis:
      rip_dominance: "recursion" | "iteration" | "praxis"
      pressure_regime: "entropy" | "equilibrium" | "extropy"
      texture_type: "placid_random" | "placid_clustered" | "disturbed" | "turbulent"
      ano_dominant: "expression" | "impression" | "adpression"
      syntegration_phase: "disintegration" | "preservation" | "integration" | "synthesis"
      transvolution_dir: "involution" | "evolution" | "balanced"
    """
    result = {}

    # RIP dominance (directly from compute_rip)
    result["rip_dominance"] = rip_result["dominance"][step]

    # Pressure regime (thresholds from design doc)
    pr = ratios[step]
    if pr > 1.2:
        result["pressure_regime"] = "entropy"
    elif pr < 0.8:
        result["pressure_regime"] = "extropy"
    else:
        result["pressure_regime"] = "equilibrium"

    # Texture type — map snapshot int to string label
    # (The snapshot texture_type is 1-4; we use it if available via ano_scores
    # approximation, but since we only have scores here, derive from pressure)
    # For transition maps, we use pressure_regime as the texture proxy at this
    # level. Full texture validation (WI-3) compares dM-variance classification.
    # Here we label based on action + pressure volatility for structural tracking.
    ano_vals = [
        ("expression", ano_scores["expression"][step]),
        ("impression", ano_scores["impression"][step]),
        ("adpression", ano_scores["adpression"][step]),
    ]
    result["ano_dominant"] = max(ano_vals, key=lambda x: x[1])[0]

    # Syntegration phase
    s_vals = [
        ("disintegration", all_scores["disintegration"][step]),
        ("preservation", all_scores["preservation"][step]),
        ("integration", all_scores["integration"][step]),
        ("synthesis", all_scores["synthesis"][step]),
    ]
    result["syntegration_phase"] = max(s_vals, key=lambda x: x[1])[0]

    # Transvolution direction
    inv = all_scores["involution"][step]
    evo = all_scores["evolution"][step]
    if abs(inv - evo) < 0.1:
        result["transvolution_dir"] = "balanced"
    elif inv > evo:
        result["transvolution_dir"] = "involution"
    else:
        result["transvolution_dir"] = "evolution"

    # Texture type from trajectory snapshot (passed via all_scores proxy)
    # We need the trajectory for this; use a simple proxy from pressure for now.
    # The full texture classification is done in WI-3 (validate_textures).
    # For transition maps, use the pressure regime as the structural state.
    result["texture_type"] = result["pressure_regime"]  # proxy until WI-3

    return result


def build_transition_map(
    all_scores: dict[str, list[float]],
    ano_scores: dict[str, list[float]],
    rip_result: dict[str, list],
    ratios: list[float],
    trajectory: Trajectory,
) -> dict[str, dict[str, Any]]:
    """Build transition count matrices for each classification axis.

    Returns dict keyed by axis name. Each value is a dict with:
      'states': list of state labels (axis-specific)
      'counts': numpy 2D array where counts[i][j] = # of transitions from i to j
      'sequence': list of state labels at each step (for path analysis)
    """
    n_steps = len(trajectory)
    if n_steps < 2:
        return {}

    # Classify every step
    step_labels = []
    for t in range(n_steps):
        step_labels.append(classify_step(all_scores, ano_scores, rip_result, ratios, t))

    # Get all axes from first step
    axes = list(step_labels[0].keys())

    result = {}
    for axis in axes:
        # Extract sequence of labels for this axis
        sequence = [step_labels[t][axis] for t in range(n_steps)]

        # Find unique states (sorted for deterministic ordering)
        states = sorted(set(sequence))
        state_to_idx = {s: i for i, s in enumerate(states)}
        n_states = len(states)

        # Count transitions
        counts = np.zeros((n_states, n_states), dtype=int)
        for t in range(n_steps - 1):
            i = state_to_idx[sequence[t]]
            j = state_to_idx[sequence[t + 1]]
            counts[i, j] += 1

        result[axis] = {
            "states": states,
            "counts": counts,
            "sequence": sequence,
        }

    return result


def transition_summary(transition_maps: dict[str, dict[str, Any]]) -> dict[str, Any]:
    """Extract structural features from transition maps.

    Returns dict with:
      absorbing_states: {axis: list of states with >50% self-transition}
      common_pathways: {axis: list of (from, to, count) top-3 transitions}
      path_entropy: {axis: Shannon entropy of transition distribution}
    """
    summary: dict[str, Any] = {
        "absorbing_states": {},
        "common_pathways": {},
        "path_entropy": {},
    }

    for axis, data in transition_maps.items():
        states = data["states"]
        counts = data["counts"]
        n = len(states)

        # Absorbing states: self-transition > 50% of outgoing
        absorbing = []
        for i in range(n):
            row_total = counts[i].sum()
            if row_total > 0 and counts[i, i] / row_total > 0.5:
                absorbing.append(states[i])
        summary["absorbing_states"][axis] = absorbing

        # Common pathways: top-3 off-diagonal transitions
        transitions = []
        for i in range(n):
            for j in range(n):
                if i != j and counts[i, j] > 0:
                    transitions.append((states[i], states[j], int(counts[i, j])))
        transitions.sort(key=lambda x: x[2], reverse=True)
        summary["common_pathways"][axis] = transitions[:3]

        # Path entropy
        flat = counts.flatten().astype(float)
        total = flat.sum()
        if total > 0:
            probs = flat / total
            probs = probs[probs > 0]
            entropy = -np.sum(probs * np.log2(probs))
        else:
            entropy = 0.0
        summary["path_entropy"][axis] = round(float(entropy), 4)

    return summary
```

**Step 4: Run tests to verify they pass**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py -v`

Expected: 4 tests PASS

**Step 5: Run full test suite to verify no regressions**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/ -q --tb=short`

Expected: 191 passed (187 existing + 4 new)

**Step 6: Commit**

```bash
git add simulator/taps_sensitivity.py tests/test_taps_sensitivity.py
git commit -m "feat: add mode transition map — categorical state classification and counting"
```

---

### Task 2: Parameter Sensitivity Sweep

Core sweep infrastructure: run TAPS mode computation across a parameter grid.

**Files:**
- Modify: `simulator/taps_sensitivity.py`
- Modify: `tests/test_taps_sensitivity.py`

**Step 1: Write failing tests for sweep_taps_modes**

Add to `tests/test_taps_sensitivity.py`:

```python
class TestSweepTapsModes(unittest.TestCase):
    """Tests for parameter sensitivity sweep."""

    def test_sweep_returns_correct_keys(self):
        """Sweep result should contain grid, mode_summaries, and sensitivity."""
        from simulator.taps_sensitivity import sweep_taps_modes

        # Tiny grid: 2 mu values, 1 alpha, 1 a value -> 2 grid points
        grid = {"mu": [0.01, 0.1], "alpha": [1e-3], "a": [8.0]}
        result = sweep_taps_modes(grid, n_agents=5, steps=20, seed=42)

        self.assertIn("grid", result)
        self.assertIn("mode_summaries", result)
        self.assertIn("sensitivity", result)

    def test_sweep_mode_summaries_shape(self):
        """Each mode summary should have one entry per grid point."""
        from simulator.taps_sensitivity import sweep_taps_modes

        grid = {"mu": [0.01, 0.05], "alpha": [1e-3], "a": [4.0]}
        result = sweep_taps_modes(grid, n_agents=5, steps=20, seed=42)

        n_points = 2 * 1 * 1  # mu * alpha * a
        for mode_name, summary in result["mode_summaries"].items():
            self.assertEqual(len(summary["mean"]), n_points,
                             f"Mode {mode_name}: expected {n_points} entries")

    def test_sensitivity_metric_computed(self):
        """Sensitivity dict should have normalized range per mode per param."""
        from simulator.taps_sensitivity import sweep_taps_modes

        grid = {"mu": [0.005, 0.05, 0.5], "alpha": [1e-3], "a": [8.0]}
        result = sweep_taps_modes(grid, n_agents=5, steps=20, seed=42)

        # Should have sensitivity for at least some modes
        self.assertGreater(len(result["sensitivity"]), 0)
        # Each mode should have sensitivity per swept param
        for mode_name, param_sens in result["sensitivity"].items():
            self.assertIn("mu", param_sens,
                          f"Mode {mode_name} should have mu sensitivity")
```

**Step 2: Run tests to verify they fail**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py::TestSweepTapsModes -v`

Expected: FAIL with `ImportError` (function not defined yet)

**Step 3: Implement sweep_taps_modes**

Add to `simulator/taps_sensitivity.py` (after the existing functions):

```python
# ---------------------------------------------------------------------------
# WI-1a: Parameter Sensitivity Sweep
# ---------------------------------------------------------------------------

def sweep_taps_modes(
    param_grid: dict[str, list[float]],
    n_agents: int = 20,
    steps: int = 120,
    seed: int = 42,
    variant: str = "logistic",
    carrying_capacity: float = 2e5,
) -> dict[str, Any]:
    """Run TAPS mode computation across a parameter grid.

    Args:
        param_grid: dict with keys from {"mu", "alpha", "a"}, each mapping
                    to a list of values to sweep. Missing keys use defaults
                    (mu=0.005, alpha=5e-3, a=3.0).
        n_agents: agents per ensemble run.
        steps: simulation steps per run.
        seed: base random seed (incremented per grid point for reproducibility).
        variant: TAP variant (default "logistic").
        carrying_capacity: K for logistic variant.

    Returns dict with:
        'grid': list of dicts, one per grid point with param values
        'mode_summaries': {mode_name: {'mean': list, 'std': list, 'final': list}}
        'sensitivity': {mode_name: {param_name: normalized_range}}
        'transition_maps': list of transition map dicts, one per grid point
    """
    from simulator.metathetic import MetatheticEnsemble

    mu_vals = param_grid.get("mu", [0.005])
    alpha_vals = param_grid.get("alpha", [5e-3])
    a_vals = param_grid.get("a", [3.0])

    grid_points = []
    all_mode_means: dict[str, list[float]] = {}
    all_mode_stds: dict[str, list[float]] = {}
    all_mode_finals: dict[str, list[float]] = {}
    all_transition_maps: list[dict] = []

    run_idx = 0
    for mu in mu_vals:
        for alpha in alpha_vals:
            for a in a_vals:
                grid_points.append({"mu": mu, "alpha": alpha, "a": a})

                # Run ensemble
                ens = MetatheticEnsemble(
                    n_agents=n_agents,
                    initial_M=10.0,
                    alpha=alpha, a=a, mu=mu,
                    variant=variant,
                    carrying_capacity=carrying_capacity if variant == "logistic" else None,
                    seed=seed + run_idx,
                    affordance_min_cluster=2,
                )
                trajectory = ens.run(steps=steps)
                run_idx += 1

                # Compute TAPS scores
                scores = compute_all_scores(trajectory, mu=mu)
                ano_scores = compute_anopression(trajectory, mu=mu)
                rip_result = compute_rip(trajectory)
                ratios = pressure_ratio(ano_scores)

                # Store per-mode summaries
                for mode_name, values in scores.items():
                    arr = np.array(values, dtype=float)
                    all_mode_means.setdefault(mode_name, []).append(float(np.nanmean(arr)))
                    all_mode_stds.setdefault(mode_name, []).append(float(np.nanstd(arr)))
                    all_mode_finals.setdefault(mode_name, []).append(float(arr[-1]) if len(arr) > 0 else 0.0)

                # Also store pressure_ratio summary
                pr_arr = np.array(ratios, dtype=float)
                all_mode_means.setdefault("pressure_ratio", []).append(float(np.nanmean(pr_arr)))
                all_mode_stds.setdefault("pressure_ratio", []).append(float(np.nanstd(pr_arr)))
                all_mode_finals.setdefault("pressure_ratio", []).append(float(pr_arr[-1]) if len(pr_arr) > 0 else 0.0)

                # Build transition map for this grid point
                t_map = build_transition_map(scores, ano_scores, rip_result, ratios, trajectory)
                all_transition_maps.append(t_map)

    # Assemble mode_summaries
    mode_summaries = {}
    for mode_name in all_mode_means:
        mode_summaries[mode_name] = {
            "mean": all_mode_means[mode_name],
            "std": all_mode_stds[mode_name],
            "final": all_mode_finals[mode_name],
        }

    # Compute sensitivity: normalized range per mode per parameter axis
    sensitivity = _compute_sensitivity(grid_points, mode_summaries, param_grid)

    return {
        "grid": grid_points,
        "mode_summaries": mode_summaries,
        "sensitivity": sensitivity,
        "transition_maps": all_transition_maps,
    }


def _compute_sensitivity(
    grid_points: list[dict],
    mode_summaries: dict,
    param_grid: dict[str, list[float]],
) -> dict[str, dict[str, float]]:
    """Compute normalized range sensitivity per mode per parameter axis.

    For each parameter that was swept (>1 value), groups grid points by
    that parameter value and computes:
      normalized_range = (max_mean - min_mean) / max(eps, overall_mean)
    """
    sensitivity: dict[str, dict[str, float]] = {}
    eps = 1e-10

    # Identify swept params (those with >1 value)
    swept_params = {k: v for k, v in param_grid.items() if len(v) > 1}

    for mode_name, summary in mode_summaries.items():
        means = np.array(summary["mean"])
        sensitivity[mode_name] = {}

        for param_name, param_values in swept_params.items():
            # For each param value, collect all mode means at that value
            per_value_means = []
            for pval in param_values:
                indices = [i for i, gp in enumerate(grid_points) if gp[param_name] == pval]
                if indices:
                    per_value_means.append(float(np.mean(means[indices])))

            if len(per_value_means) >= 2:
                range_val = max(per_value_means) - min(per_value_means)
                overall_mean = abs(float(np.mean(means)))
                sensitivity[mode_name][param_name] = round(
                    range_val / max(eps, overall_mean), 4
                )

    return sensitivity
```

**Step 4: Run tests to verify they pass**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py -v`

Expected: 7 tests PASS (4 old + 3 new)

**Step 5: Run full test suite**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/ -q --tb=short`

Expected: 194 passed

**Step 6: Commit**

```bash
git add simulator/taps_sensitivity.py tests/test_taps_sensitivity.py
git commit -m "feat: add TAPS parameter sensitivity sweep with per-mode metrics"
```

---

### Task 3: Gated vs Ungated Divergence Finder

**Files:**
- Modify: `simulator/taps_sensitivity.py`
- Modify: `tests/test_taps_sensitivity.py`

**Step 1: Write failing tests**

Add to `tests/test_taps_sensitivity.py`:

```python
class TestDivergenceMap(unittest.TestCase):
    """Tests for gated vs ungated divergence."""

    def test_identical_params_zero_divergence(self):
        """Same gate settings should produce zero divergence."""
        from simulator.taps_sensitivity import compute_divergence

        # Run with same gate setting twice -> divergence should be minimal
        grid = {"mu": [0.01], "alpha": [1e-3], "a": [8.0]}
        result = compute_divergence(
            grid, n_agents=5, steps=20, seed=42,
            gated_cluster=2, ungated_cluster=2,  # SAME setting
        )
        for mode_name, divs in result["mode_divergence"].items():
            for d in divs:
                self.assertAlmostEqual(d, 0.0, places=5,
                    msg=f"Mode {mode_name} should have 0 divergence with same gate")

    def test_divergence_returns_all_modes(self):
        """Divergence result should cover all TAPS modes."""
        from simulator.taps_sensitivity import compute_divergence

        grid = {"mu": [0.01], "alpha": [1e-3], "a": [8.0]}
        result = compute_divergence(grid, n_agents=5, steps=20, seed=42)
        # Should have at least the 17 TAPS modes
        self.assertGreaterEqual(len(result["mode_divergence"]), 17)
```

**Step 2: Run tests to verify they fail**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py::TestDivergenceMap -v`

Expected: FAIL with `ImportError`

**Step 3: Implement compute_divergence**

Add to `simulator/taps_sensitivity.py`:

```python
# ---------------------------------------------------------------------------
# WI-2: Gated vs Ungated Divergence
# ---------------------------------------------------------------------------

def compute_divergence(
    param_grid: dict[str, list[float]],
    n_agents: int = 20,
    steps: int = 120,
    seed: int = 42,
    variant: str = "logistic",
    carrying_capacity: float = 2e5,
    gated_cluster: int = 2,
    ungated_cluster: int = 0,
) -> dict[str, Any]:
    """Compute gated-vs-ungated TAPS mode differences across parameter space.

    At each grid point, runs BOTH gated and ungated with same seed, then
    computes per-mode mean absolute difference over time steps.

    Returns dict with:
        'grid': list of param dicts
        'mode_divergence': {mode_name: list of mean-abs-diff per grid point}
        'transition_divergence': {axis: list of Frobenius norm per grid point}
        'significant_regimes': list of grid points where any mode divergence > threshold
    """
    from simulator.metathetic import MetatheticEnsemble

    mu_vals = param_grid.get("mu", [0.005])
    alpha_vals = param_grid.get("alpha", [5e-3])
    a_vals = param_grid.get("a", [3.0])

    grid_points = []
    mode_divergence: dict[str, list[float]] = {}
    transition_divergence: dict[str, list[float]] = {}
    significant_regimes = []

    run_idx = 0
    for mu in mu_vals:
        for alpha in alpha_vals:
            for a in a_vals:
                gp = {"mu": mu, "alpha": alpha, "a": a}
                grid_points.append(gp)

                shared_seed = seed + run_idx
                run_idx += 1

                # Run gated
                ens_g = MetatheticEnsemble(
                    n_agents=n_agents, initial_M=10.0,
                    alpha=alpha, a=a, mu=mu, variant=variant,
                    carrying_capacity=carrying_capacity if variant == "logistic" else None,
                    seed=shared_seed, affordance_min_cluster=gated_cluster,
                )
                traj_g = ens_g.run(steps=steps)

                # Run ungated
                ens_u = MetatheticEnsemble(
                    n_agents=n_agents, initial_M=10.0,
                    alpha=alpha, a=a, mu=mu, variant=variant,
                    carrying_capacity=carrying_capacity if variant == "logistic" else None,
                    seed=shared_seed, affordance_min_cluster=ungated_cluster,
                )
                traj_u = ens_u.run(steps=steps)

                # Compute scores for both
                scores_g = compute_all_scores(traj_g, mu=mu)
                scores_u = compute_all_scores(traj_u, mu=mu)

                # Per-mode mean absolute difference
                max_div = 0.0
                for mode_name in scores_g:
                    arr_g = np.array(scores_g[mode_name], dtype=float)
                    arr_u = np.array(scores_u[mode_name], dtype=float)
                    min_len = min(len(arr_g), len(arr_u))
                    if min_len > 0:
                        mad = float(np.mean(np.abs(arr_g[:min_len] - arr_u[:min_len])))
                    else:
                        mad = 0.0
                    mode_divergence.setdefault(mode_name, []).append(mad)
                    max_div = max(max_div, mad)

                # Transition map divergence
                ano_g = compute_anopression(traj_g, mu=mu)
                ano_u = compute_anopression(traj_u, mu=mu)
                rip_g = compute_rip(traj_g)
                rip_u = compute_rip(traj_u)
                ratios_g = pressure_ratio(ano_g)
                ratios_u = pressure_ratio(ano_u)

                tmap_g = build_transition_map(scores_g, ano_g, rip_g, ratios_g, traj_g)
                tmap_u = build_transition_map(scores_u, ano_u, rip_u, ratios_u, traj_u)

                # Frobenius norm of transition count difference per axis
                for axis in tmap_g:
                    if axis in tmap_u:
                        # Align state dimensions
                        all_states = sorted(set(tmap_g[axis]["states"]) | set(tmap_u[axis]["states"]))
                        n_s = len(all_states)
                        state_map = {s: i for i, s in enumerate(all_states)}

                        mat_g = np.zeros((n_s, n_s))
                        mat_u = np.zeros((n_s, n_s))

                        for i, sg in enumerate(tmap_g[axis]["states"]):
                            for j, sg2 in enumerate(tmap_g[axis]["states"]):
                                mat_g[state_map[sg], state_map[sg2]] = tmap_g[axis]["counts"][i, j]

                        for i, su in enumerate(tmap_u[axis]["states"]):
                            for j, su2 in enumerate(tmap_u[axis]["states"]):
                                mat_u[state_map[su], state_map[su2]] = tmap_u[axis]["counts"][i, j]

                        frob = float(np.linalg.norm(mat_g - mat_u))
                        transition_divergence.setdefault(axis, []).append(frob)

                if max_div > 0.1:
                    significant_regimes.append(gp)

    return {
        "grid": grid_points,
        "mode_divergence": mode_divergence,
        "transition_divergence": transition_divergence,
        "significant_regimes": significant_regimes,
    }
```

**Step 4: Run tests to verify they pass**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py -v`

Expected: 9 tests PASS

**Step 5: Run full test suite**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/ -q --tb=short`

Expected: 196 passed

**Step 6: Commit**

```bash
git add simulator/taps_sensitivity.py tests/test_taps_sensitivity.py
git commit -m "feat: add gated vs ungated divergence finder across parameter space"
```

---

### Task 4: Texture Validation & Correlation Stability

**Files:**
- Modify: `simulator/taps_sensitivity.py`
- Modify: `tests/test_taps_sensitivity.py`

**Step 1: Write failing tests**

Add to `tests/test_taps_sensitivity.py`:

```python
class TestTextureValidation(unittest.TestCase):
    """Tests for dM-variance texture classification."""

    def test_classify_dM_texture_length(self):
        """Output length should match trajectory length."""
        from simulator.taps_sensitivity import classify_dM_texture

        traj = _make_trajectory(steps=20, with_events=True)
        result = classify_dM_texture(traj, window=5)
        self.assertEqual(len(result), 20)

    def test_classify_dM_texture_values_in_range(self):
        """All texture types should be in {0, 1, 2, 3, 4} (0 = unclassified)."""
        from simulator.taps_sensitivity import classify_dM_texture

        traj = _make_trajectory(steps=20, with_events=True)
        result = classify_dM_texture(traj, window=5)
        for t_type in result:
            self.assertIn(t_type, {0, 1, 2, 3, 4})


class TestCorrelationStability(unittest.TestCase):
    """Tests for correlation stability across parameter space."""

    def test_all_identical_returns_fully_stable(self):
        """If all runs are identical, correlated pairs should be 100% stable."""
        from simulator.taps_sensitivity import correlation_stability

        # Simulate sweep results where every grid point has same correlation
        # Create mock mode_summaries isn't needed; we need the raw scores.
        # Use sweep_taps_modes with a single grid point repeated.
        from simulator.taps_sensitivity import sweep_taps_modes
        grid = {"mu": [0.01], "alpha": [1e-3], "a": [8.0]}
        sweep = sweep_taps_modes(grid, n_agents=5, steps=30, seed=42)

        result = correlation_stability(sweep, threshold=0.85)
        self.assertIn("stability_map", result)
        self.assertIn("stable_pairs", result)
```

**Step 2: Run tests to verify they fail**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py::TestTextureValidation tests/test_taps_sensitivity.py::TestCorrelationStability -v`

Expected: FAIL

**Step 3: Implement classify_dM_texture and correlation_stability**

Add to `simulator/taps_sensitivity.py`:

```python
# ---------------------------------------------------------------------------
# WI-3: Texture Type Validation
# ---------------------------------------------------------------------------

def classify_dM_texture(
    trajectory: Trajectory,
    window: int = 10,
) -> list[int]:
    """Classify Emery/Trist texture type from rolling dM variance and mean.

    Returns list of texture type (1-4) per step. Steps within the initial
    window are labeled 0 (unclassified).

    Classification (from taps_mapping.md Section 6):
      Type 1 (placid-randomized): low variance, low |dM|
      Type 2 (placid-clustered): low variance, positive dM
      Type 3 (disturbed-reactive): medium variance
      Type 4 (turbulent): high variance
    """
    n = len(trajectory)
    if n < 2:
        return [0] * n

    # Compute dM series
    dM = [0.0]
    for i in range(1, n):
        dM.append(trajectory[i]["total_M"] - trajectory[i - 1]["total_M"])

    result = [0] * n

    for t in range(window, n):
        window_dM = dM[t - window + 1 : t + 1]
        var = float(np.var(window_dM))
        mean_dM = float(np.mean(window_dM))

        # Adaptive thresholds based on the overall dM scale
        overall_var = float(np.var(dM[1:])) if len(dM) > 1 else 1.0
        overall_var = max(overall_var, 1e-10)

        var_ratio = var / overall_var

        if var_ratio > 1.5:
            result[t] = 4  # Turbulent
        elif var_ratio > 0.5:
            result[t] = 3  # Disturbed-reactive
        elif mean_dM > 0:
            result[t] = 2  # Placid-clustered
        else:
            result[t] = 1  # Placid-randomized

    return result


def validate_textures(
    trajectory: Trajectory,
    window: int = 10,
) -> dict[str, Any]:
    """Compare environment-derived and dM-derived texture classifications.

    Returns:
        env_texture: list[int] from snapshot texture_type field
        dM_texture: list[int] from rolling dM variance
        agreement_rate: float (fraction matching, excluding unclassified steps)
        confusion_matrix: numpy 5x5 array (rows=env, cols=dM, index 0=unclassified)
    """
    dM_texture = classify_dM_texture(trajectory, window=window)
    env_texture = [snap.get("texture_type", 0) for snap in trajectory]

    # Compute agreement (only for classified steps)
    matches = 0
    compared = 0
    for e, d in zip(env_texture, dM_texture):
        if d > 0:  # skip unclassified
            compared += 1
            if e == d:
                matches += 1

    agreement = matches / max(1, compared)

    # Confusion matrix (5x5: index 0 = unclassified, 1-4 = types)
    confusion = np.zeros((5, 5), dtype=int)
    for e, d in zip(env_texture, dM_texture):
        confusion[e, d] += 1

    return {
        "env_texture": env_texture,
        "dM_texture": dM_texture,
        "agreement_rate": round(agreement, 4),
        "confusion_matrix": confusion,
    }


# ---------------------------------------------------------------------------
# WI-4: Correlation Stability
# ---------------------------------------------------------------------------

def correlation_stability(
    sweep_result: dict[str, Any],
    threshold: float = 0.85,
) -> dict[str, Any]:
    """Determine if correlated mode pairs are stable across parameter space.

    Takes output from sweep_taps_modes. For each grid point, recomputes
    the correlation matrix from the stored trajectory scores, then tracks
    which pairs remain correlated across all grid points.

    Note: This function re-runs the correlation analysis at each grid point.
    Since sweep_taps_modes stores only summaries (mean/std/final), we need
    to re-run the ensemble to get full score series. For efficiency, we
    use the transition maps as a proxy: pairs that show synchronized
    transitions are likely correlated.

    However, for accuracy, this function uses a lightweight approach:
    it runs small ensembles at each grid point to get score series.

    Returns:
        stable_pairs: list of (mode_a, mode_b) correlated at >80% of grid points
        unstable_pairs: list of (mode_a, mode_b) correlated at <50%
        stability_map: dict of {(mode_a, mode_b): fraction}
        param_dependent: dict of {(mode_a, mode_b): list of params where broken}
    """
    from simulator.metathetic import MetatheticEnsemble
    from simulator.taps import correlation_matrix as taps_corr_matrix

    grid = sweep_result["grid"]
    n_points = len(grid)

    if n_points == 0:
        return {"stable_pairs": [], "unstable_pairs": [],
                "stability_map": {}, "param_dependent": {}}

    # Track correlation presence per pair
    pair_counts: dict[tuple[str, str], int] = {}
    pair_param_absent: dict[tuple[str, str], list[dict]] = {}

    for gp in grid:
        # Re-run a small ensemble at this grid point
        ens = MetatheticEnsemble(
            n_agents=10, initial_M=10.0,
            alpha=gp["alpha"], a=gp["a"], mu=gp["mu"],
            variant="logistic", carrying_capacity=2e5,
            seed=42, affordance_min_cluster=2,
        )
        traj = ens.run(steps=60)  # shorter for speed
        scores = compute_all_scores(traj, mu=gp["mu"])
        corr = taps_corr_matrix(scores)

        # Record which pairs are correlated at this point
        correlated_here = set()
        for a, b, r in corr["highly_correlated"]:
            pair = (min(a, b), max(a, b))
            correlated_here.add(pair)
            pair_counts[pair] = pair_counts.get(pair, 0) + 1

        # Record where pairs are NOT correlated
        for pair in pair_counts:
            if pair not in correlated_here:
                pair_param_absent.setdefault(pair, []).append(gp)

    # Classify stability
    stable = []
    unstable = []
    stability_map = {}

    for pair, count in pair_counts.items():
        frac = count / n_points
        stability_map[pair] = round(frac, 4)
        if frac > 0.8:
            stable.append(pair)
        elif frac < 0.5:
            unstable.append(pair)

    return {
        "stable_pairs": stable,
        "unstable_pairs": unstable,
        "stability_map": stability_map,
        "param_dependent": {k: v for k, v in pair_param_absent.items()},
    }
```

**Step 4: Run tests to verify they pass**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/test_taps_sensitivity.py -v`

Expected: 13 tests PASS

**Step 5: Run full test suite**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/ -q --tb=short`

Expected: 200 passed

**Step 6: Commit**

```bash
git add simulator/taps_sensitivity.py tests/test_taps_sensitivity.py
git commit -m "feat: add texture validation and correlation stability analysis"
```

---

### Task 5: CLI Driver Script + Figures

The CLI driver runs everything and generates the 5 figures listed in the design.

**Files:**
- Create: `scripts/taps_stage2.py`

**Step 1: Implement CLI driver**

```python
# scripts/taps_stage2.py
"""TAPS Stage 2: comparative and sensitivity analysis.

CLAIM POLICY LABEL: exploratory
Runs parameter sensitivity sweep, mode transition maps, gated vs ungated
divergence analysis, texture validation, and correlation stability.

Produces:
  outputs/figures/taps_sensitivity_heatmap.png
  outputs/figures/taps_transition_maps.png
  outputs/figures/taps_divergence_map.png
  outputs/figures/taps_texture_validation.png
  outputs/figures/taps_correlation_stability.png

Usage:
  python scripts/taps_stage2.py
  python scripts/taps_stage2.py --quick           (reduced grid for fast iteration)
  python scripts/taps_stage2.py --steps 200       (longer runs)
"""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np

from simulator.taps_sensitivity import (
    build_transition_map,
    classify_dM_texture,
    compute_divergence,
    correlation_stability,
    sweep_taps_modes,
    transition_summary,
    validate_textures,
)
from simulator.taps import (
    compute_all_scores,
    compute_anopression,
    compute_rip,
    pressure_ratio,
)

ROOT = Path(__file__).resolve().parents[1]
FIG_OUT = ROOT / "outputs" / "figures"


def default_grid() -> dict[str, list[float]]:
    """Full parameter grid: ~240 points."""
    return {
        "mu": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1],
        "alpha": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3],
        "a": [2.0, 4.0, 8.0, 16.0, 32.0],
    }


def quick_grid() -> dict[str, list[float]]:
    """Reduced grid for fast iteration: ~24 points."""
    return {
        "mu": [1e-3, 1e-2, 1e-1],
        "alpha": [1e-4, 1e-3, 5e-3],
        "a": [4.0, 8.0],
    }


# --- Figure generation functions ---

def fig_sensitivity_heatmap(sweep_result: dict, save_path: Path) -> None:
    """Parameter x Mode sensitivity heatmap."""
    plt.rcParams.update({"font.size": 9, "savefig.dpi": 300, "savefig.bbox": "tight"})

    sensitivity = sweep_result["sensitivity"]
    if not sensitivity:
        print("  No sensitivity data to plot")
        return

    # Get all modes and all swept params
    modes = sorted(sensitivity.keys())
    params = sorted({p for s in sensitivity.values() for p in s})

    if not params:
        print("  No swept parameters to plot")
        return

    # Build matrix
    matrix = np.zeros((len(modes), len(params)))
    for i, mode in enumerate(modes):
        for j, param in enumerate(params):
            matrix[i, j] = sensitivity[mode].get(param, 0.0)

    fig, ax = plt.subplots(figsize=(max(6, len(params) * 2), max(8, len(modes) * 0.4)))
    im = ax.imshow(matrix, cmap="YlOrRd", aspect="auto")

    ax.set_xticks(range(len(params)))
    ax.set_xticklabels(params, fontsize=8)
    ax.set_yticks(range(len(modes)))
    ax.set_yticklabels(modes, fontsize=7)

    for i in range(len(modes)):
        for j in range(len(params)):
            val = matrix[i, j]
            color = "white" if val > 0.5 * matrix.max() else "black"
            ax.text(j, i, f"{val:.2f}", ha="center", va="center", fontsize=6, color=color)

    plt.colorbar(im, ax=ax, label="Normalized Range")
    ax.set_title("TAPS Mode Sensitivity to Parameters  [exploratory]")
    ax.set_xlabel("Parameter")
    ax.set_ylabel("Mode")

    save_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(str(save_path))
    plt.close(fig)
    print(f"  Wrote {save_path}")


def fig_transition_maps(sweep_result: dict, save_path: Path) -> None:
    """Directed graph representation of mode transitions (uses first grid point)."""
    plt.rcParams.update({"font.size": 9, "savefig.dpi": 300, "savefig.bbox": "tight"})

    if not sweep_result["transition_maps"]:
        print("  No transition maps to plot")
        return

    # Use first grid point as representative
    tmap = sweep_result["transition_maps"][0]
    axes = list(tmap.keys())
    n_axes = len(axes)

    fig, axs = plt.subplots(2, 3, figsize=(15, 10))
    axs = axs.flatten()

    for idx, axis in enumerate(axes[:6]):
        ax = axs[idx]
        data = tmap[axis]
        states = data["states"]
        counts = data["counts"]
        n = len(states)

        # Simple heatmap representation of transition matrix
        im = ax.imshow(counts, cmap="Blues", aspect="auto")
        ax.set_xticks(range(n))
        ax.set_yticks(range(n))
        ax.set_xticklabels(states, rotation=45, ha="right", fontsize=7)
        ax.set_yticklabels(states, fontsize=7)
        ax.set_title(axis, fontsize=9)
        ax.set_xlabel("To")
        ax.set_ylabel("From")

        # Annotate cells
        for i in range(n):
            for j in range(n):
                if counts[i, j] > 0:
                    color = "white" if counts[i, j] > counts.max() * 0.5 else "black"
                    ax.text(j, i, str(counts[i, j]), ha="center", va="center",
                            fontsize=7, color=color)

    # Hide unused axes
    for idx in range(len(axes), len(axs)):
        axs[idx].set_visible(False)

    fig.suptitle("Mode Transition Maps (first grid point)  [exploratory]", fontsize=11)
    plt.tight_layout()
    save_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(str(save_path))
    plt.close(fig)
    print(f"  Wrote {save_path}")


def fig_divergence_map(div_result: dict, save_path: Path) -> None:
    """Gated vs ungated divergence across parameter space."""
    plt.rcParams.update({"font.size": 9, "savefig.dpi": 300, "savefig.bbox": "tight"})

    mode_div = div_result["mode_divergence"]
    grid = div_result["grid"]

    if not grid or not mode_div:
        print("  No divergence data to plot")
        return

    # Top-10 modes by max divergence
    mode_max = {m: max(d) for m, d in mode_div.items()}
    top_modes = sorted(mode_max, key=mode_max.get, reverse=True)[:10]

    fig, ax = plt.subplots(figsize=(max(8, len(grid) * 0.3), 6))

    x = range(len(grid))
    for mode in top_modes:
        ax.plot(x, mode_div[mode], marker=".", label=mode, markersize=3)

    ax.axhline(y=0.1, color="red", linestyle="--", alpha=0.5, label="threshold")
    ax.set_xlabel("Grid point index")
    ax.set_ylabel("Mean |gated - ungated|")
    ax.set_title("Gated vs Ungated Divergence (top 10 modes)  [exploratory]")
    ax.legend(fontsize=7, ncol=2)

    save_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(str(save_path))
    plt.close(fig)
    print(f"  Wrote {save_path}")


def fig_texture_validation(traj: list[dict], save_path: Path) -> None:
    """Texture validation: confusion matrix and dM variance distributions."""
    plt.rcParams.update({"font.size": 9, "savefig.dpi": 300, "savefig.bbox": "tight"})

    result = validate_textures(traj, window=10)
    confusion = result["confusion_matrix"]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Left: confusion matrix
    labels = ["unclass", "placid-rand", "placid-clust", "disturbed", "turbulent"]
    im = ax1.imshow(confusion, cmap="Blues", aspect="auto")
    ax1.set_xticks(range(5))
    ax1.set_yticks(range(5))
    ax1.set_xticklabels(labels, rotation=45, ha="right", fontsize=7)
    ax1.set_yticklabels(labels, fontsize=7)
    ax1.set_xlabel("dM-derived")
    ax1.set_ylabel("Environment-derived")
    ax1.set_title(f"Texture Agreement: {result['agreement_rate']:.1%}")

    for i in range(5):
        for j in range(5):
            if confusion[i, j] > 0:
                ax1.text(j, i, str(confusion[i, j]), ha="center", va="center", fontsize=8)

    plt.colorbar(im, ax=ax1)

    # Right: dM variance over time
    dM = [0.0]
    for i in range(1, len(traj)):
        dM.append(traj[i]["total_M"] - traj[i - 1]["total_M"])

    ax2.plot(dM, color="steelblue", linewidth=0.8)
    ax2.set_xlabel("Step")
    ax2.set_ylabel("dM")
    ax2.set_title("dM trajectory")

    # Color-code background by dM-derived texture
    texture_colors = {0: "white", 1: "#E3F2FD", 2: "#BBDEFB", 3: "#FFE0B2", 4: "#FFCCBC"}
    for t in range(len(result["dM_texture"])):
        ax2.axvspan(t - 0.5, t + 0.5, color=texture_colors[result["dM_texture"][t]], alpha=0.3)

    fig.suptitle("Texture Type Validation  [exploratory]", fontsize=11)
    plt.tight_layout()
    save_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(str(save_path))
    plt.close(fig)
    print(f"  Wrote {save_path}")


def fig_correlation_stability(corr_result: dict, save_path: Path) -> None:
    """Correlation stability across parameter space."""
    plt.rcParams.update({"font.size": 9, "savefig.dpi": 300, "savefig.bbox": "tight"})

    stability_map = corr_result["stability_map"]

    if not stability_map:
        print("  No correlation stability data to plot")
        return

    pairs = sorted(stability_map.keys())
    fracs = [stability_map[p] for p in pairs]
    pair_labels = [f"{a}\n{b}" for a, b in pairs]

    fig, ax = plt.subplots(figsize=(max(8, len(pairs) * 0.6), 5))
    colors = ["#4CAF50" if f > 0.8 else "#F44336" if f < 0.5 else "#FFC107" for f in fracs]
    bars = ax.bar(range(len(pairs)), fracs, color=colors)

    ax.axhline(y=0.8, color="green", linestyle="--", alpha=0.5, label="stable threshold")
    ax.axhline(y=0.5, color="red", linestyle="--", alpha=0.5, label="unstable threshold")
    ax.set_xticks(range(len(pairs)))
    ax.set_xticklabels(pair_labels, fontsize=6, rotation=45, ha="right")
    ax.set_ylabel("Fraction of grid points correlated")
    ax.set_title("Mode Correlation Stability Across Parameter Space  [exploratory]")
    ax.legend(fontsize=8)
    ax.set_ylim(0, 1.05)

    save_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(str(save_path))
    plt.close(fig)
    print(f"  Wrote {save_path}")


def print_stage2_summary(sweep: dict, div: dict, corr_stab: dict) -> None:
    """Print Stage 2 analysis summary."""
    print(f"\n{'='*60}")
    print(f"  TAPS Stage 2 Analysis Summary")
    print(f"{'='*60}")

    # Sensitivity highlights
    print(f"\n  Parameter Sensitivity (top 5 most sensitive modes):")
    all_sens = {}
    for mode, params in sweep["sensitivity"].items():
        for param, val in params.items():
            all_sens[(mode, param)] = val
    top5 = sorted(all_sens.items(), key=lambda x: x[1], reverse=True)[:5]
    for (mode, param), val in top5:
        print(f"    {mode} / {param}: {val:.4f}")

    # Divergence highlights
    sig = div.get("significant_regimes", [])
    print(f"\n  Gated vs Ungated Divergence:")
    print(f"    Significant regimes: {len(sig)} / {len(div.get('grid', []))}")
    if sig:
        print(f"    First significant: {sig[0]}")

    # Transition map summary (first grid point)
    if sweep["transition_maps"]:
        from simulator.taps_sensitivity import transition_summary
        ts = transition_summary(sweep["transition_maps"][0])
        print(f"\n  Mode Transition Summary (first grid point):")
        for axis, absorbing in ts["absorbing_states"].items():
            if absorbing:
                print(f"    {axis}: absorbing states = {absorbing}")
        for axis, entropy in ts["path_entropy"].items():
            print(f"    {axis}: path entropy = {entropy:.2f} bits")

    # Correlation stability
    print(f"\n  Correlation Stability:")
    print(f"    Stable pairs (>80%): {len(corr_stab.get('stable_pairs', []))}")
    print(f"    Unstable pairs (<50%): {len(corr_stab.get('unstable_pairs', []))}")
    for pair, frac in sorted(corr_stab.get("stability_map", {}).items(), key=lambda x: x[1], reverse=True):
        print(f"    {pair[0]} <-> {pair[1]}: {frac:.0%}")

    print(f"\n  [exploratory — see CLAIM_POLICY.md]")


def main() -> None:
    parser = argparse.ArgumentParser(description="TAPS Stage 2 analysis")
    parser.add_argument("--quick", action="store_true", help="Use reduced grid (~24 points)")
    parser.add_argument("--steps", type=int, default=120, help="Steps per run")
    parser.add_argument("--n-agents", type=int, default=10, help="Agents per run")
    parser.add_argument("--seed", type=int, default=42, help="Base random seed")
    args = parser.parse_args()

    grid = quick_grid() if args.quick else default_grid()
    n_points = 1
    for v in grid.values():
        n_points *= len(v)
    print(f"TAPS Stage 2: {n_points} grid points, {args.steps} steps, {args.n_agents} agents")

    # WI-1a + WI-1b: Parameter sweep with transition maps
    print("\n[1/4] Running parameter sensitivity sweep...")
    sweep = sweep_taps_modes(grid, n_agents=args.n_agents, steps=args.steps, seed=args.seed)
    fig_sensitivity_heatmap(sweep, FIG_OUT / "taps_sensitivity_heatmap.png")
    fig_transition_maps(sweep, FIG_OUT / "taps_transition_maps.png")

    # WI-2: Divergence
    print("\n[2/4] Running gated vs ungated divergence analysis...")
    div = compute_divergence(grid, n_agents=args.n_agents, steps=args.steps, seed=args.seed)
    fig_divergence_map(div, FIG_OUT / "taps_divergence_map.png")

    # WI-3: Texture validation (run single representative trajectory)
    print("\n[3/4] Running texture validation...")
    from simulator.metathetic import MetatheticEnsemble
    ens = MetatheticEnsemble(
        n_agents=args.n_agents, initial_M=10.0,
        alpha=5e-3, a=3.0, mu=0.005,
        variant="logistic", carrying_capacity=2e5,
        seed=args.seed, affordance_min_cluster=2,
    )
    traj = ens.run(steps=args.steps)
    fig_texture_validation(traj, FIG_OUT / "taps_texture_validation.png")

    # WI-4: Correlation stability
    print("\n[4/4] Running correlation stability analysis...")
    corr_stab = correlation_stability(sweep, threshold=0.85)
    fig_correlation_stability(corr_stab, FIG_OUT / "taps_correlation_stability.png")

    # Summary
    print_stage2_summary(sweep, div, corr_stab)
    print("\nDone.")


if __name__ == "__main__":
    main()
```

**Step 2: Run the CLI with --quick to test end-to-end**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" scripts/taps_stage2.py --quick --steps 40`

Expected: All 5 figures generated, summary printed, no errors.

**Step 3: Run full test suite to confirm no regressions**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/ -q --tb=short`

Expected: All tests pass (200+)

**Step 4: Commit**

```bash
git add scripts/taps_stage2.py
git commit -m "feat: add TAPS Stage 2 CLI with sensitivity, divergence, and transition figures"
```

---

### Task 6: Empirical Targets Research Document

Research and documentation task — no code.

**Files:**
- Create: `docs/empirical_targets.md`

**Step 1: Research and write the document**

Use web search to identify specific datasets and data sources in each of the 5 domains listed in the design (organizational ecology, innovation economics, evolutionary biology, chemistry/nuclear physics, plus assessment per domain).

The document should include:
- **CLAIM POLICY LABEL: exploratory** header
- One section per domain
- Specific dataset names, sources, access methods
- Mapping difficulty assessment (how hard to connect to sigma-TAP)
- Priority ranking
- Bohr periodic table parallel note (per project author's observation)

**Step 2: Commit**

```bash
git add docs/empirical_targets.md
git commit -m "docs: add empirical targets research for future validation"
```

---

### Task 7: Final Verification & Push

**Step 1: Run full test suite**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" -m pytest tests/ -v`

Expected: All tests pass (~200)

**Step 2: Run Stage 2 CLI with full grid (if time permits)**

Run: `"C:\Users\user\AppData\Local\Programs\Python\Python312\python.exe" scripts/taps_stage2.py --steps 120`

Expected: All 5 figures generated, meaningful results.

**Step 3: Push to GitHub**

```bash
git push origin unified-integration-20260225
```

**Step 4: Verify figures visually**

Check all 5 new figures in `outputs/figures/` for readability and correctness.
